{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Import 'spacy' for stopwords and 'string' for punctuation."
      ],
      "metadata": {
        "id": "I2EFTUhVvIlO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXiIZaOSB9cN"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from heapq import nlargest\n",
        "\n",
        "# Loading the text from the 'spaceX_DP.txt' file\n",
        "with open('spaceX_DP.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Tokenize the 'SpaceX.txt' corpus.\n"
      ],
      "metadata": {
        "id": "uK9m3R1PvIi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the English tokenizer and stop words\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#Tokenizing the text and removing stopwords and punctuation\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc if token.text not in STOP_WORDS and token.text not in string.punctuation]\n"
      ],
      "metadata": {
        "id": "5rwbjlKalNeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Build 'word frequency'. Make sure you have removed the stopwords.\n",
        "\n",
        "• Determine the maximum frequency as 'word_frequencies[word]=\n",
        "(word_frequencies[word]/maximum_frequency).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xDoEifjlvIfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building word frequency dictionary\n",
        "word_frequencies = defaultdict(int)\n",
        "for word in tokens:\n",
        "    word_frequencies[word] += 1\n",
        "\n",
        "maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "for word in word_frequencies:\n",
        "    word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n"
      ],
      "metadata": {
        "id": "4Q04ARaPlTKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Tokenize the sentences. Generate the sentence_scores. Score every sentence\n",
        "based on number of words.\n"
      ],
      "metadata": {
        "id": "43wNdcEDvIdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "# Generating sentence scores based on the number of words in each sentence\n",
        "sentence_scores = {}\n",
        "for sentence in sentences:\n",
        "    for word in sentence:\n",
        "        if word.text in word_frequencies:\n",
        "            if sentence not in sentence_scores:\n",
        "                sentence_scores[sentence] = word_frequencies[word.text]\n",
        "            else:\n",
        "                sentence_scores[sentence] += word_frequencies[word.text]\n"
      ],
      "metadata": {
        "id": "dKgqPZvplqtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Import nlargest from heapq and provide 'summarized_sentences'.\n"
      ],
      "metadata": {
        "id": "PaUp038WvIYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest\n",
        "\n",
        "# number of sentences for the summary (e.g., 5 sentences)\n",
        "num_summary_sentences = 5\n",
        "\n",
        "#summarized sentences using nlargest\n",
        "summarized_sentences = nlargest(num_summary_sentences, sentence_scores, key=sentence_scores.get)\n"
      ],
      "metadata": {
        "id": "S7bV6tBRl8Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Convert sentences from spacy to strings and join all sentences.\n"
      ],
      "metadata": {
        "id": "3M8BjA4GvIRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = ' '.join([str(sentence) for sentence in summarized_sentences])\n"
      ],
      "metadata": {
        "id": "nJHtJm2WnEFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "• Determine the length of the summary.\n",
        "\n",
        "• Determine the length of the original text."
      ],
      "metadata": {
        "id": "yD57AYPRuQ1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summary_length = len(summary)\n",
        "original_text_length = len(text)\n"
      ],
      "metadata": {
        "id": "T5CSTe6SnJCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide a summary once you have converted spacy outputs to strings.\n",
        "\n",
        "• Use spacy and textrank to summarize the text.\n",
        "\n",
        "• Print the summary in 5 sentences."
      ],
      "metadata": {
        "id": "euNbdgWXvIH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "# Loading the English language model and adding the 'sentencizer' component\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe('sentencizer')\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "word_freq = Counter(word.text.lower() for word in doc if word.text not in STOP_WORDS and word.text not in punctuation)\n",
        "sentence_scores = []\n",
        "for sentence in sentences:\n",
        "    sentence_scores.append((sentence, sum(word_freq[word.text.lower()] for word in nlp(sentence))))\n",
        "\n",
        "sentence_scores = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Selecting the top 5 sentences as the summary\n",
        "num_summary_sentences = 5\n",
        "summary_sentences = [sentence[0] for sentence in sentence_scores[:num_summary_sentences]]\n",
        "\n",
        "summary = ' '.join(summary_sentences)\n",
        "\n",
        "# Printing the summary in 5 sentences\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVLQdTq8nM73",
        "outputId": "bbd8b249-283a-4cfb-d588-7fc5d23fc469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "However, the regulatory agency that is was supposed to be guiding this environmental impact process, the FAA, allowed SpaceX.”Five years after the FAA issued its Final Environmental Impact Statement, the agency issued a 23-page “Written Re-Evaluation of the 2014 Final Environmental Impact Statement for the SpaceX Texas Launch Site,” on May 21, 2019, that acknowledged SpaceX had switched from the Falcon program to the Starship project that included a new “experimental test program.” The metamorphosis of this facility, which sits on tender tidal flats and feet from sand dunes where sea turtles lay eggs, has largely gone unnoticed and under the radar in this border community where environmentalists say they are strapped between fighting the construction of Donald Trump’s border wall through the region, and the development of three new liquefied natural gas facilities at the deepwater Port of Brownsville just 5 miles from SpaceX. More hangars and buildings have been built 1.5 miles from the test launch pad, where SpaceX has its administrative offices, and more and more cars line the sand dunes across from the complex, and unless one travels often the 21-mile stretch of Highway 4, also known as Boca Chica Highway, that leads to the beach and passes by the SpaceX complex, it might not be so noticeable. The FAA responded on July 17 that a new Environmental Assessment (EA) is in fact underway, though Jim Chapman, president of Friends of the Wildlife Corridor and the main drafter of the July 3 letter to the FAA, told The Brownsville Herald that an EA is insufficient considering the magnitude of SpaceX’s impact and that SpaceX should perform the more rigorous Environmental Impact Statement (EIS) instead. Environmental organizations spotlight inadequate oversight by FAA of SpaceX’s Boca Chica testing facility, call for an EIS In a letter sent to the Federal Aviation Administratio(FAA), environmental groups are calling on the agency to require a new Environmental Impact Statement (EIS) that fully details the impact of the SpaceX Boca Chica testing and launch site. Border Report asked the Federal Aviation Administration (FAA,) the agency in charge of oversight of this facility, whether current SpaceX activities comply with agency regulations stipulated in the FAA’s 2014 final Environmental Impact Statement (EIS), which allowed SpaceX to build its Texas Launch Facility a half-mile from Boca Chica Beach on the Gulf Coast in an ecological wetland where sea turtles nest in rural Cameron County.\n"
          ]
        }
      ]
    }
  ]
}