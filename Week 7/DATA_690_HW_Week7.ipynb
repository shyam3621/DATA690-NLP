{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1. Sentiment Analysis\n",
        "\n",
        "      • Using textblob, what is the probability that the sentiment in the Burbank text is going to negative?"
      ],
      "metadata": {
        "id": "DPB2Wnxsjek6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "file_path = 'Burbank.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    burbank_text = file.read()\n",
        "\n",
        "# Creating a TextBlob object\n",
        "blob = TextBlob(burbank_text)\n",
        "\n",
        "sentiment_polarity = blob.sentiment.polarity\n",
        "\n",
        "# Checking the probability that the sentiment is negative\n",
        "probability_negative = 1 - (sentiment_polarity + 1) / 2\n",
        "\n",
        "print(\"Sentiment Polarity:\", sentiment_polarity)\n",
        "print(\"Probability of Negative Sentiment:\", probability_negative)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OGbXuFWjRkx",
        "outputId": "0ead9f84-e858-41f3-df81-e1984ba836f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Polarity: 0.09869334480780263\n",
            "Probability of Negative Sentiment: 0.45065332759609866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2. Sentiment Analysis\n",
        "\n",
        "    • Using the data from exercise 1 and textblob, what is the overall sentiment and subjectivity?"
      ],
      "metadata": {
        "id": "nOwgTrv9GeJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "file_path = 'Burbank.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    burbank_text = file.read()\n",
        "\n",
        "blob = TextBlob(burbank_text)\n",
        "\n",
        "# Getting the overall sentiment and subjectivity\n",
        "overall_sentiment = \"Positive\" if blob.sentiment.polarity > 0 else \"Negative\" if blob.sentiment.polarity < 0 else \"Neutral\"\n",
        "overall_subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "print(\"Overall Sentiment:\", overall_sentiment)\n",
        "print(\"Overall Subjectivity:\", overall_subjectivity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfNRvy9jjvHL",
        "outputId": "b7d86725-54c5-4c25-e16a-4b5716e45a0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Sentiment: Positive\n",
            "Overall Subjectivity: 0.3790877796901893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3.\n",
        "    \n",
        "    Key topic using ‘Word’ from textblob (very simple way to determine the key topics) based on the Burbank text file.\n",
        "    \n",
        "    Import Word from textblob. Identify the key topics by using Word from textblob."
      ],
      "metadata": {
        "id": "78XVheWLGmzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Downloading the necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AACIgY_Ut8mv",
        "outputId": "e49ce8e6-9630-4ccc-d2de-8e7e3ead3299"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "file_path = 'Burbank.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    burbank_text = file.read()\n",
        "\n",
        "blob = TextBlob(burbank_text)\n",
        "\n",
        "# Extracting nouns to consider the topic\n",
        "nouns = [word for (word, pos) in blob.tags if pos.startswith('N')]\n",
        "\n",
        "# Lemmatizing the nouns\n",
        "lemmatized_nouns = [Word(word).lemmatize() for word in nouns]\n",
        "\n",
        "noun_frequency = Counter(lemmatized_nouns)\n",
        "\n",
        "# Getting the top 5 key topics based on noun frequency\n",
        "top_nouns = noun_frequency.most_common(5)\n",
        "\n",
        "print(\"Top 5 Key Topics:\")\n",
        "for noun, frequency in top_nouns:\n",
        "    print(f\"{noun}: {frequency} occurrences\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exxSRy45tu13",
        "outputId": "4d4f380c-92b0-4b4f-96fd-3153d46ccb8a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Key Nouns:\n",
            "flight: 15 occurrences\n",
            "FAA: 13 occurrences\n",
            "Burbank: 12 occurrences\n",
            "Valley: 11 occurrences\n",
            "recommendation: 9 occurrences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 4. Sentiment analysis with spaCy.\n",
        "\n",
        "    • Load the datasets ‘amazon_cells_labelled.txt’, ‘imdb_labelled.txt’, ‘yelp_labelled.txt’\n",
        "    • Create ‘combined_col’ by joining the tables such that combined_col=[data_amazon, data_imdb, data_yelp]\n",
        "    • Check the structure of data_amazon\n",
        "    • Add headers for columns in each dataset: ‘Review’ and ‘Label’\n",
        "    • Create a “Company’ column to identify each company ‘Amazon’,\n",
        "    ‘imdb’, and ‘yelp’\n",
        "    • Explore the structure of the new dataset called ‘comb_data’\n",
        "    • Use ‘comb_data.to_csv’ to create the ‘Sentiment_Analysis_Dataset’\n",
        "    • Print the columns\n",
        "    • Check for null values\n",
        "    • Import STOP_WORDS from spacy and stopwords from\n",
        "    spacy.lang.en.stop_words\n",
        "    • Build a list of stopwords for filtering\n",
        "    • Import string, define ‘punctuations’ and define a ‘parser’\n",
        "    • Tokenize the sentences"
      ],
      "metadata": {
        "id": "0siBF0U8G0jI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import string\n",
        "\n",
        "# Loading the datasets\n",
        "amazon_data = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', header=None, names=['Review', 'Label'])\n",
        "imdb_data = pd.read_csv('imdb_labelled.txt', sep='\\t', header=None, names=['Review', 'Label'])\n",
        "yelp_data = pd.read_csv('yelp_labelled.txt', sep='\\t', header=None, names=['Review', 'Label'])\n",
        "\n",
        "# Combined dataset\n",
        "combined_col = [amazon_data, imdb_data, yelp_data]\n",
        "\n",
        "#structure of data_amazon\n",
        "print(\"Structure of data_amazon:\")\n",
        "print(amazon_data.head())\n",
        "\n",
        "#headers for columns\n",
        "for dataset in combined_col:\n",
        "    dataset.columns = ['Review', 'Label']\n",
        "\n",
        "# Creating'Company' column\n",
        "for i, dataset in enumerate(combined_col):\n",
        "    dataset['Company'] = ['Amazon', 'IMDb', 'Yelp'][i]\n",
        "\n",
        "comb_data = pd.concat(combined_col, ignore_index=True)\n",
        "\n",
        "#structure of the new dataset\n",
        "print(\"\\nStructure of comb_data:\")\n",
        "print(comb_data.head())\n",
        "\n",
        "# Saving 'comb_data' to CSV\n",
        "comb_data.to_csv('Sentiment_Analysis_Dataset.csv', index=False)\n",
        "\n",
        "print(\"\\nColumns of comb_data:\")\n",
        "print(comb_data.columns)\n",
        "\n",
        "print(\"\\nNull values in comb_data:\")\n",
        "print(comb_data.isnull().sum())\n",
        "\n",
        "# Importing STOP_WORDS from spaCy\n",
        "stop_words = STOP_WORDS\n",
        "\n",
        "# Importing string, define 'punctuations' and defining a 'parser'\n",
        "punctuations = string.punctuation\n",
        "parser = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Tokenize the sentences\n",
        "def tokenize_sentence(sentence):\n",
        "    tokens = parser(sentence)\n",
        "    tokens = [token.text.lower() for token in tokens if token.text.lower() not in stop_words and token.text not in punctuations]\n",
        "    return tokens\n",
        "\n",
        "# Example usage\n",
        "example_sentence = comb_data.Review[3]\n",
        "tokenized_example = tokenize_sentence(example_sentence)\n",
        "print(\"\\nTokenized example sentence:\")\n",
        "print(tokenized_example)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8HM39A-zTOb",
        "outputId": "8cbe0267-adc7-4a6e-c9ef-b06bc2f2edf6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structure of data_amazon:\n",
            "                                              Review  Label\n",
            "0  So there is no way for me to plug it in here i...      0\n",
            "1                        Good case, Excellent value.      1\n",
            "2                             Great for the jawbone.      1\n",
            "3  Tied to charger for conversations lasting more...      0\n",
            "4                                  The mic is great.      1\n",
            "\n",
            "Structure of comb_data:\n",
            "                                              Review  Label Company\n",
            "0  So there is no way for me to plug it in here i...      0  Amazon\n",
            "1                        Good case, Excellent value.      1  Amazon\n",
            "2                             Great for the jawbone.      1  Amazon\n",
            "3  Tied to charger for conversations lasting more...      0  Amazon\n",
            "4                                  The mic is great.      1  Amazon\n",
            "\n",
            "Columns of comb_data:\n",
            "Index(['Review', 'Label', 'Company'], dtype='object')\n",
            "\n",
            "Null values in comb_data:\n",
            "Review     0\n",
            "Label      0\n",
            "Company    0\n",
            "dtype: int64\n",
            "\n",
            "Tokenized example sentence:\n",
            "['tied', 'charger', 'conversations', 'lasting', '45', 'minutes', 'major', 'problems']\n"
          ]
        }
      ]
    }
  ]
}